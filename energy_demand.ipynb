{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "IwKV7uHNDPiK",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "iFMNqcu4MITA",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2441,
     "status": "ok",
     "timestamp": 1575376072328,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "M-fSfna9cbFL",
    "outputId": "895f5e65-c0bb-4fd0-903d-a0e1f6fcbd97"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os,random, math, psutil, pickle \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import missingno as msno\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "import math\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "pd.set_option('max_columns', 150)\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Import modelling libs\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate, BatchNormalization, Flatten, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error as mse_loss\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Ekg3u_1eDPiQ"
   },
   "source": [
    "### Reducing Memory Size Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "qmfxHz5tDPiQ"
   },
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Description Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/al146/OneDrive - Heriot-Watt University/Data/Findhorn/test/'\n",
    "\n",
    "# creats a list of all files with .csv format in above path.\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "# a function for merging csv files and saving the clean result\n",
    "def mergeCSV(fileslist):\n",
    "    result = pd.DataFrame()\n",
    "    for file in fileslist:\n",
    "        table = pd.DataFrame(columns=['file_id'])\n",
    "        table.loc[len(table)] = file\n",
    "        filecsv = open(file)\n",
    "        table['total_no'] = len(filecsv.readlines())\n",
    "        df = pd.read_csv(file, names=['date', 'time', 'meter'])\n",
    "        table['zero_no'] = sum((df == 0).sum(axis=1))\n",
    "        table['na_no'] = sum(pd.isnull(df['meter']))\n",
    "        table['zero%'] = (table['zero_no']/table['total_no'])*100\n",
    "        table['na%'] = (table['na_no']/table['total_no'])*100\n",
    "        result = result.append(table, ignore_index=True)\n",
    "    result.file_id.replace({'C:/Users/al146/OneDrive - Heriot-Watt University/Data/Findhorn/test/':''}, regex=True, inplace=True)\n",
    "    result.file_id.replace({'_15Feb2015_28Mar2015.csv':''}, regex=True, inplace=True)\n",
    "    result.to_csv('csv/zero_na_no.csv')\n",
    "\n",
    "mergeCSV(files)\n",
    "\n",
    "zero_na_no = pd.read_csv('csv/zero_na_no.csv')\n",
    "zero_na_no.drop(zero_na_no.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Merging the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# a functon for replacing the date format of excel with python\n",
    "def read_date(date):\n",
    "    return xlrd.xldate.xldate_as_datetime(date, 0)\n",
    "\n",
    "# a function for merging csv files and saving the clean result\n",
    "def mergeCSV(fileslist):\n",
    "    result = pd.DataFrame()\n",
    "    for file in fileslist:\n",
    "        table = pd.read_csv(file, names=['date', 'time', 'meter_reading'], skiprows=1)\n",
    "        table[\"timestamp\"] = table[\"date\"] + \" \" + table[\"time\"].astype(str)\n",
    "        table['timestamp'] = pd.to_datetime(table['timestamp'], errors='raise', infer_datetime_format=True)\n",
    "        table = table.drop(['time', 'date'], axis=1)\n",
    "        table['site_id'] = file\n",
    "        result = result.append(table, ignore_index=True)\n",
    "    #result.site_id.replace({'_\\(2015-02-15,42days\\).csv':''}, regex=True, inplace=True)\n",
    "    result.site_id.replace({'_15Feb2015_28Mar2015.csv':''}, regex=True, inplace=True)\n",
    "    result.site_id.replace({'C:/Users/al146/OneDrive - Heriot-Watt University/Data/Findhorn/test/':''}, regex=True, inplace=True)\n",
    "    #result.site_id.replace({'C:/Users/al146/Desktop/New folder/Processed-':''}, regex=True, inplace=True)\n",
    "    result.site_id.replace({'D_1Ph_':''}, regex=True, inplace=True)\n",
    "    result.site_id.replace({'_mf':''}, regex=True, inplace=True)\n",
    "    result.to_csv('csv/merge_data.csv')\n",
    "mergeCSV(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "del r, f, d, file, files, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Creating one house data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_c32 = pd.read_csv('C:/Users/al146/OneDrive - Heriot-Watt University/Data/Findhorn/test/C32_15Feb2015_28Mar2015.csv', names=['date', 'time', 'meter_reading'], skiprows=1)\n",
    "data_c32[\"timestamp\"] = data_c32[\"date\"] + \" \" + data_c32[\"time\"].astype(str)\n",
    "data_c32['timestamp'] = pd.to_datetime(data_c32['timestamp'], errors='raise', infer_datetime_format=True)\n",
    "data_c32 = data_c32.drop(['time', 'date'], axis=1)\n",
    "data_c32['site_id'] = 'c32'\n",
    "data_c32.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "weatherdata = pd.read_csv('csv/weather20092019.csv')\n",
    "weatherdata.columns = weatherdata.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# filitering to the station 132\n",
    "weatherdata = weatherdata[weatherdata['src_id']==132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed:0', 'min_grss_temp', 'min_conc_temp', 'min_grss_temp_q', 'min_conc_temp_q', 'meto_stmp_time', 'midas_stmp_etime', 'min_grss_temp_j', 'min_conc_temp_j', 'max_air_temp_q',\n",
    "            'min_air_temp_q', 'max_air_temp_j', 'min_air_temp_j', 'id_type', 'id', 'ob_hour_count', 'version_num', 'met_domain_name', 'src_id', 'rec_st_ind']\n",
    "weatherdata = weatherdata.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weatherdata['date'] = pd.to_datetime(weatherdata['ob_end_time'])\n",
    "\n",
    "weatherdata['max_air_temp'] = pd.to_numeric(weatherdata['max_air_temp'], errors='coerce')\n",
    "weatherdata['min_air_temp'] = pd.to_numeric(weatherdata['min_air_temp'], errors='coerce')\n",
    "\n",
    "# adding a mean column\n",
    "weatherdata['mean_air_temp'] = weatherdata.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather_df = weatherdata[['date','max_air_temp', 'min_air_temp', 'mean_air_temp']].set_index('date').resample('D').mean().reset_index()\n",
    "\n",
    "del weatherdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather_df.to_pickle('pkl/weather_df.pkl')\n",
    "del weather_df, drop_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "CH4QknwwDPiT",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ufY2DYC8DPiX"
   },
   "source": [
    "### Reading and cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "6iUWVTIMc8I5"
   },
   "outputs": [],
   "source": [
    "data_df = reduce_mem_usage(pd.read_csv('csv/merge_data.csv'))\n",
    "holidays_df = reduce_mem_usage(pd.read_csv('csv/UKholidays.csv'))\n",
    "weather_df = reduce_mem_usage(pd.read_pickle('pkl/weather_df.pkl'))\n",
    "\n",
    "data_df.drop(data_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "data_df['site_id'] = data_df['site_id'].astype('str')\n",
    "data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], errors='raise')\n",
    "holidays_df['date'] = pd.to_datetime(holidays_df['date'], errors='raise', infer_datetime_format=True)\n",
    "\n",
    "# replace negative values with zero\n",
    "data_df['meter_reading'].mask(data_df['meter_reading'] < 0, 0.09, inplace=True)\n",
    "data_c32['meter_reading'].mask(data_c32['meter_reading'] < 0, 0.09, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Aggregating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_agg_int = pd.DataFrame()\n",
    "for i, id in enumerate(data_df['site_id'].value_counts().index.to_list()):\n",
    "    data_agg = data_df[data_df['site_id'] == id][['timestamp', 'meter_reading']].set_index('timestamp').resample('D').sum().reset_index()\n",
    "    data_agg['site_id'] = np.str(id)\n",
    "    data_agg_int = data_agg_int.append(data_agg, ignore_index=True)\n",
    "\n",
    "data_agg = data_agg_int.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "del i, id, data_agg_int, data_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "n1jqmfi7DPi-"
   },
   "source": [
    "### Adding datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "opnm5M8lDPjB"
   },
   "outputs": [],
   "source": [
    "data_df['month'] = data_df['timestamp'].dt.month.astype(np.int8)\n",
    "data_df['week_of_year'] = data_df['timestamp'].dt.weekofyear.astype(np.int8)\n",
    "data_df['day_of_year'] = data_df['timestamp'].dt.dayofyear.astype(np.int16)\n",
    "data_df['hour_of_day'] = data_df['timestamp'].dt.hour.astype(np.int8)  \n",
    "data_df['day_of_week'] = data_df['timestamp'].dt.dayofweek.astype(np.int8)\n",
    "data_df['day_of_month'] = data_df['timestamp'].dt.day.astype(np.int8)\n",
    "data_df['week_of_month'] = data_df['timestamp'].dt.day/7\n",
    "data_df['week_of_month'] = data_df['week_of_month'].apply(lambda x: math.ceil(x)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_c32['month'] = data_c32['timestamp'].dt.month.astype(np.int8)\n",
    "data_c32['week_of_year'] = data_c32['timestamp'].dt.weekofyear.astype(np.int8)\n",
    "data_c32['day_of_year'] = data_c32['timestamp'].dt.dayofyear.astype(np.int16)\n",
    "data_c32['hour_of_day'] = data_c32['timestamp'].dt.hour.astype(np.int8)  \n",
    "data_c32['day_of_week'] = data_c32['timestamp'].dt.dayofweek.astype(np.int8)\n",
    "data_c32['day_of_month'] = data_c32['timestamp'].dt.day.astype(np.int8)\n",
    "data_c32['week_of_month'] = data_c32['timestamp'].dt.day/7\n",
    "data_c32['week_of_month'] = data_c32['week_of_month'].apply(lambda x: math.ceil(x)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Convert data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1575376116550,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "9WRpXwAonAmZ",
    "outputId": "7aa830b2-5e76-448b-f06c-9e5d3b6d26f7"
   },
   "outputs": [],
   "source": [
    "# list of site_ids\n",
    "data_df.site_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1204,
     "status": "ok",
     "timestamp": 1575376160071,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "aUNAf3-erFkj",
    "outputId": "cce3275e-0a9d-478f-994a-11e91005ce78"
   },
   "outputs": [],
   "source": [
    "convert_dict = {'site_id' : str,\n",
    "                'month': int,\n",
    "                'week_of_year': int,\n",
    "                'day_of_year': int,\n",
    "                'hour_of_day': int,\n",
    "                'day_of_week': int,\n",
    "                'day_of_month': int,\n",
    "                'week_of_month': int,\n",
    "                'meter_reading': float\n",
    "               }\n",
    "\n",
    "data_df = data_df.astype(convert_dict)\n",
    "data_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "-jT3_bocDPif"
   },
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1575376114808,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "nFfo-uPIdKFa",
    "outputId": "594ba576-f50e-44c0-80cf-176eb47208d4"
   },
   "outputs": [],
   "source": [
    "data_df.info()\n",
    "holidays_df.info()\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1872,
     "status": "ok",
     "timestamp": 1575376116544,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "Thz1psXZDPil",
    "outputId": "a83f6240-093e-4ee3-8c46-681ca1f102be"
   },
   "outputs": [],
   "source": [
    "print('Size of train_df data', data_df.shape)\n",
    "print('Size of train_df data', holidays_df.shape)\n",
    "print('Size of train_df data', weather_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1575376116545,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "FdSXZ5XkDPin",
    "outputId": "f899aa19-7e9f-49df-d717-0645f634353a"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1575376116548,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "lF3bOQc4DPis",
    "outputId": "9cd0f478-725e-47da-f460-2f21ed17a9cb"
   },
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "rEaNmGUoDPjz"
   },
   "source": [
    "## Saving the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1449,
     "status": "ok",
     "timestamp": 1575376163633,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "S9R4nplhDPj0",
    "outputId": "baa45b19-bfe5-434e-b46b-f0363b8e5967"
   },
   "outputs": [],
   "source": [
    "data_df.to_pickle('pkl/data_df.pkl')\n",
    "holidays_df.to_pickle('pkl/holidays_df.pkl')\n",
    "data_c32.to_pickle('pkl/data_c32.pkl')\n",
    "\n",
    "del data_df\n",
    "del holidays_df\n",
    "del weather_df\n",
    "del data_c32\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "FoUxQN1qbKTZ",
    "toc-hr-collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1575376165580,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "Bqs8WZWeDPj4",
    "outputId": "6ef1289d-d24b-4443-d8d9-d9c26deef16c"
   },
   "outputs": [],
   "source": [
    "# Loading the pickle file\n",
    "eda_df = pd.read_pickle('pkl/data_df.pkl')\n",
    "#eda_df.drop(columns=['month', 'week_of_year', 'day_of_year', 'hour_of_day', 'day_of_week', 'day_of_month', 'week_of_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "xofOc6i_DPjF",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Plots for non-aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "n4tl89_7DPjJ"
   },
   "source": [
    "### Daily and hourly demand for each building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53088,
     "status": "error",
     "timestamp": 1575366136385,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "DWWOB_rlOZjC",
    "outputId": "644cf3b3-1acb-4350-b839-769192c77c0f"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))\n",
    "\n",
    "for i, id in enumerate(eda_df['site_id'].value_counts().index.to_list()):\n",
    "    eda_df[eda_df['site_id'] == id][['timestamp', 'meter_reading']].set_index('timestamp').resample('H').mean()['meter_reading'].plot(ax=axes[i%4][i//4], alpha=0.3, label='By Hour', color='blue').set_ylabel('Mean meter reading', fontsize=10);\n",
    "    eda_df[eda_df['site_id'] == id][['timestamp', 'meter_reading']].set_index('timestamp').resample('D').mean()['meter_reading'].plot(ax=axes[i%4][i//4], alpha=1, label='By day', color='black').set_xlabel('');\n",
    "    axes[i%4][i//4].grid(color='black', alpha=0.5, linestyle='dashed', linewidth=0.5)\n",
    "    axes[i%4][i//4].patch.set_facecolor('white')\n",
    "    axes[i%4][i//4].legend();\n",
    "    axes[i%4][i//4].set_title(id, fontsize=12);\n",
    "    plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4CpCxKa-DPjG"
   },
   "source": [
    "### Missing data and zeros visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3431,
     "status": "ok",
     "timestamp": 1575366156830,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "7tf_7T1fDPjJ",
    "outputId": "6378eb7b-d742-4718-8fdd-147e954a1875"
   },
   "outputs": [],
   "source": [
    "# Load data to another df\n",
    "train_sns = eda_df.set_index(['timestamp'])\n",
    "\n",
    "# change the site_id to numbers first\n",
    "train_sns['site_id'] = train_sns.site_id.replace(['A01', 'A03', 'A18', 'B02', 'B08', 'C02', 'C14', 'C19',\n",
    "       'C24', 'C32', 'D08', 'D15', 'D20', 'D25', 'F27'], [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "\n",
    "f,ax=plt.subplots(1,1,figsize=(10,20))\n",
    "df = train_sns.copy().reset_index()\n",
    "df['timestamp'] = df[\"timestamp\"].values.astype('timedelta64[ns]')\n",
    "df['timestamp'] = pd.to_timedelta(df.timestamp).dt.total_seconds() / 3600\n",
    "df['timestamp'] = df.timestamp.astype(int)\n",
    "df.timestamp -= df.timestamp.min()\n",
    "\n",
    "# create a new column for sites with no energy reading\n",
    "df['nodata'] = 0\n",
    "df.nodata[(df['meter_reading'].isnull())] = 1\n",
    "\n",
    "# sum the meter reading for each hour\n",
    "df = df.groupby(['timestamp', 'site_id']).sum()\n",
    "df = df.reset_index()\n",
    "\n",
    "missmap = np.empty((15, df.timestamp.max()+1))\n",
    "missmap.fill(np.nan)\n",
    "for l in df.values:\n",
    "    if l[3]>=1 and l[2]==0:\n",
    "        missmap[int(l[1]), int(l[0])] = 0\n",
    "    elif l[2]==0:\n",
    "        missmap[int(l[1]), int(l[0])] = 0.5\n",
    "    else:\n",
    "        missmap[int(l[1]), int(l[0])] = 1\n",
    "\n",
    "# Define colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = ('gainsboro', 'gray', 'royalblue')\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', colors, len(colors))\n",
    "\n",
    "sns.heatmap(missmap, cmap=cmap, ax=ax, cbar=True, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Set the colorbar labels\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_ticks([0.15, 0.5, 0.85])\n",
    "colorbar.set_ticklabels(['meter reading with no value', 'meter reading available with zero value', 'meter reading available with non-zero value'])\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_ylabel('site_id')    \n",
    "ax.set_xlabel('hours elapsed since')\n",
    "ax.set_title('Missing data and zeros visualized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "hIrqSJ0xDPjM"
   },
   "source": [
    "### Average meter_reading by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1575366194041,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "UE5LXyBhDPjN",
    "outputId": "d72bfb3d-9e64-4040-96dd-ab2ea85f1dd0"
   },
   "outputs": [],
   "source": [
    "del train_sns\n",
    "\n",
    "train_data = eda_df['hour_of_day'].value_counts(dropna=False, normalize=True).sort_index().values\n",
    "ind = np.arange(len(train_data))\n",
    "width = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n",
    "tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "axes.set_ylabel('Normalized number of observations');\n",
    "axes.set_xlabel('Hour');\n",
    "axes.set_xticks(ind + width / 2)\n",
    "axes.set_xticklabels(eda_df['hour_of_day'].value_counts().sort_index().index, rotation=0)\n",
    "axes2 = axes.twinx()\n",
    "mr = axes2.plot(ind, eda_df[['hour_of_day', 'meter_reading']].groupby('hour_of_day')['meter_reading'].mean().sort_index().values, 'D-', color='tab:orange', label='Mean meter reading');\n",
    "axes2.grid(False);\n",
    "axes2.tick_params(axis='y', labelcolor='tab:orange');\n",
    "axes2.set_ylabel('Mean meter reading by hour', color='tab:orange');\n",
    "axes.legend([tr], ['Train'], facecolor='white');\n",
    "axes2.legend(loc=2, facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "A8b0AF6wDPjP"
   },
   "source": [
    "### Average meter_reading by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3022,
     "status": "ok",
     "timestamp": 1575366194692,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "hPC49k6cDPjP",
    "outputId": "4bfea54e-7bb2-451c-9fb3-cba378e2c533"
   },
   "outputs": [],
   "source": [
    "train_data = eda_df['day_of_month'].value_counts(dropna=False, normalize=True).sort_index().values\n",
    "ind = np.arange(len(train_data))\n",
    "width = 0.35\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n",
    "tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "axes.set_ylabel('Normalized number of observations');\n",
    "axes.set_xlabel('day');\n",
    "axes.set_xticks(ind + width / 2)\n",
    "axes.set_xticklabels(eda_df['day_of_month'].value_counts().sort_index().index, rotation=0)\n",
    "axes2 = axes.twinx()\n",
    "mr = axes2.plot(ind, eda_df[['day_of_month', 'meter_reading']].groupby('day_of_month')['meter_reading'].mean().sort_index().values, 'D-', color='tab:orange', label='Mean meter reading');\n",
    "axes2.grid(False);\n",
    "axes2.tick_params(axis='y', labelcolor='tab:orange');\n",
    "axes2.set_ylabel('Mean meter reading by day of month', color='tab:orange');\n",
    "axes.legend([tr], ['Train'], facecolor='white');\n",
    "axes2.legend(loc=2, facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "MmlV22vyDPjR"
   },
   "source": [
    "### Average meter_reading by weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2189,
     "status": "ok",
     "timestamp": 1575367338476,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "AVx222p3DPjS",
    "outputId": "a3fce8c2-3a30-427a-da6d-1813111e4827"
   },
   "outputs": [],
   "source": [
    "train_data = eda_df['day_of_week'].value_counts(dropna=False, normalize=True).sort_index().values\n",
    "ind = np.arange(len(train_data))\n",
    "width = 0.4\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n",
    "tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "axes.set_ylabel('Normalized number of observations');\n",
    "axes.set_xlabel('weekday');\n",
    "axes.set_xticks(ind + width / 2)\n",
    "axes2 = axes.twinx()\n",
    "mr = axes2.plot(ind, eda_df[['day_of_week', 'meter_reading']].groupby('day_of_week')['meter_reading'].mean().sort_index().values, 'D-', color='tab:orange', label='Mean meter reading');\n",
    "axes2.grid(False);\n",
    "axes2.tick_params(axis='y', labelcolor='tab:orange');\n",
    "axes2.set_ylabel('Mean meter reading by day of week', color='tab:orange');\n",
    "axes.legend([tr], ['Train'], facecolor='white');\n",
    "axes2.legend(loc=2, facecolor='white');\n",
    "\n",
    "axes.set_xticklabels(eda_df['day_of_week'].value_counts().sort_index().index.map({0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}), rotation=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "PRk3Er0BDPjU"
   },
   "source": [
    "### Average meter_reading by site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2026,
     "status": "ok",
     "timestamp": 1575367319196,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "bBfLKI20DPjV",
    "outputId": "ccd3c157-378b-44cb-b7bc-a6cd6fce8b4c"
   },
   "outputs": [],
   "source": [
    "train_data = eda_df['site_id'].value_counts(dropna=False, normalize=True).sort_index().values\n",
    "ind = np.arange(len(train_data))\n",
    "width = 0.4\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(18, 6), dpi=100)\n",
    "tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "axes.set_ylabel('Normalized number of observations');\n",
    "axes.set_xlabel('site_id');\n",
    "axes.set_xticks(ind + width / 2)\n",
    "axes.set_xticklabels(eda_df['site_id'].value_counts().sort_index().index, rotation=0)\n",
    "axes2 = axes.twinx()\n",
    "mr = axes2.plot(ind, eda_df[['site_id', 'meter_reading']].groupby('site_id')['meter_reading'].mean().sort_index().values, 'D-', color='tab:orange', label='Mean meter reading');\n",
    "axes2.grid(False);\n",
    "axes2.tick_params(axis='y', labelcolor='tab:orange');\n",
    "axes2.set_ylabel('Mean meter reading by site_id', color='tab:orange');\n",
    "axes.legend([tr], ['Train'], facecolor='white');\n",
    "axes2.legend(loc=2, facecolor='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "dwgEKVDfDPjX"
   },
   "source": [
    "### Box plot of meter_readig by site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1575367378242,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "5NiuCsoBDPjb",
    "outputId": "ccf10e82-ec8d-4aeb-9e47-6962b899f899"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(18, 6))\n",
    "sns.boxplot(x='site_id', y='meter_reading', data=eda_df, showfliers=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Je7Ry7ZLDPje"
   },
   "source": [
    "### Number of observations by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1575367404182,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "Vh_UBgSuDPje",
    "outputId": "cdcd1c76-6806-4477-8d42-8303c620e3ab"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(14, 6))\n",
    "eda_df['timestamp'].dt.floor('d').value_counts().sort_index().plot(ax=axes).set_xlabel('Date', fontsize=14);\n",
    "axes.set_title('Number of observations by day', fontsize=16);\n",
    "axes.legend(['Train']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ctLwsLRUDPjg"
   },
   "source": [
    "### Amount of data and NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1575367523212,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "XjEF4llFDPj8",
    "outputId": "4022db6a-8cb4-427e-cf89-85c4fabc6819"
   },
   "outputs": [],
   "source": [
    "total = eda_df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (eda_df.isnull().sum()/eda_df.isnull().count()*100).sort_values(ascending = False)\n",
    "missing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing__train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1448,
     "status": "ok",
     "timestamp": 1575367464002,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "qQ32JsYdDPjh",
    "outputId": "af4aba37-522a-427d-88eb-ed71f2260158"
   },
   "outputs": [],
   "source": [
    "train_data = (eda_df.count() / len(eda_df)).drop('meter_reading').sort_values().values\n",
    "ind = np.arange(len(train_data))\n",
    "width = 0.35\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(14, 6), dpi=100)\n",
    "tr = axes.bar(ind, train_data, width, color='royalblue')\n",
    "\n",
    "axes.set_ylabel('Amount of data available');\n",
    "axes.set_xticks(ind + width / 2)\n",
    "axes.set_xticklabels((eda_df.count() / len(eda_df)).drop('meter_reading').sort_values().index, rotation=40)\n",
    "axes.legend([tr], ['Train']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1742,
     "status": "ok",
     "timestamp": 1575367950510,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "Nz6jhwmUDPkT",
    "outputId": "b4af631b-e4bf-48f4-e40d-7bcd7b85b28b"
   },
   "outputs": [],
   "source": [
    "msno.matrix(eda_df.head(200), figsize=(15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1840,
     "status": "ok",
     "timestamp": 1575367977929,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "XXupWXQBPssD",
    "outputId": "dfd10f30-9527-4a0a-8e28-e6d386da4cd4"
   },
   "outputs": [],
   "source": [
    "msno.bar(eda_df.head(200), figsize=(15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1330,
     "status": "ok",
     "timestamp": 1575367530177,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "uXcE1llgafw9",
    "outputId": "112605cb-8a1e-4208-b6da-3d9886968218"
   },
   "outputs": [],
   "source": [
    "a = msno.heatmap(eda_df, sort='ascending', figsize=(15,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1575368050669,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "A4DczFMaQJfY",
    "outputId": "993cc83d-0e19-4fbb-c234-05c172a43473"
   },
   "outputs": [],
   "source": [
    "msno.dendrogram(eda_df, figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "IsDlMXuoa4QB"
   },
   "source": [
    "## Examine the Distribution of the Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1575367494028,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "RYndLmnpDPkH",
    "outputId": "43587acb-476b-494d-95ce-5b652d434a2b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "eda_df['meter_reading'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1575367647997,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "4Py9l6A1DPkJ",
    "outputId": "4e3a0cba-8824-49bd-900b-f739ed7ca6a5"
   },
   "outputs": [],
   "source": [
    "eda_df['meter_reading'].plot(kind='hist',\n",
    "                            bins=10,\n",
    "                            figsize=(10, 5),\n",
    "                           title='Distribution of Target Variable (meter_reading)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1575367499128,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "z2MAZ17HDPkK",
    "outputId": "fe06ccb1-db4b-45dc-ddae-f21c22e93acb"
   },
   "outputs": [],
   "source": [
    "#Target's log-log histogram:\n",
    "\n",
    "ax = np.log1p(eda_df['meter_reading']).hist()\n",
    "ax.set_yscale('log')\n",
    "eda_df.meter_reading.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "rwJ2AlQ8Jqyb"
   },
   "source": [
    "## Outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1575368432338,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "LtLeKeJnDPkX",
    "outputId": "52ed5225-082d-4b17-df45-cd4746575266"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "_ = stats.probplot(eda_df['meter_reading'], fit=True, rvalue=True, plot=plt)\n",
    "#plt.title(\"Probability plot for meter_reading shows extreme skewness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1575367548744,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "5aSLwRaOHE4t",
    "outputId": "637b696d-83cc-4e48-f53d-08bdcb238ad3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(eda_df.meter_reading, hist=False)\n",
    "#plt.title(f\"Target variable meter_reading is highly skewed\")\n",
    "plt.title(\"Energy demand\")\n",
    "plt.ylabel(\"Count of readings\")\n",
    "plt.xlabel(f\"Measured consumption\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(np.log1p(eda_df.meter_reading), hist=False)\n",
    "#plt.title(f\"After log transform, distributions of energy types look comparably skewed\")\n",
    "plt.title(\"Log transform of energy demand\")\n",
    "plt.ylabel(\"Count of readings\")\n",
    "plt.xlabel(f\"Measured consumption\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2580,
     "status": "ok",
     "timestamp": 1575367581635,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "o9iYq0lzj4t8",
    "outputId": "c5a715a5-142f-4c7f-ac5b-e298fb5492f8"
   },
   "outputs": [],
   "source": [
    "y_mean_time = eda_df.groupby('timestamp').meter_reading.mean()\n",
    "y_mean_time.plot(figsize=(15, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2182,
     "status": "ok",
     "timestamp": 1575367593098,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "HXWoDEgbcIUm",
    "outputId": "e7b7a507-2fd8-428c-d57c-2b8b9983b191"
   },
   "outputs": [],
   "source": [
    "y_mean_time.rolling(window=10).std().plot(figsize=(15, 3))\n",
    "plt.axhline(y=0.1, color='red')\n",
    "plt.axvspan(0, 23, color='green', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11104,
     "status": "ok",
     "timestamp": 1575367563284,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "6aGfpHDVHTm8",
    "outputId": "9cb674ea-97e9-49b1-e024-9b2c5ee3c174"
   },
   "outputs": [],
   "source": [
    "train_df2 = eda_df.copy()\n",
    "\n",
    "# change the site_id to numbers first\n",
    "train_df2['site_id'] = train_df2.site_id.replace(['A01', 'A03', 'A18', 'B02', 'B08', 'C02', 'C14', 'C19',\n",
    "       'C24', 'C32', 'D08', 'D15', 'D20', 'D25', 'F27'], [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "\n",
    "# for first four\n",
    "for bldg_id in [0, 1, 2, 3]:\n",
    "    plt.figure(figsize=(15,3))\n",
    "    tmp_df = train_df2[train_df2.site_id == bldg_id].copy()\n",
    "    tmp_df.set_index(\"timestamp\", inplace=True)\n",
    "    tmp_df.resample(\"D\").meter_reading.sum().plot()\n",
    "    plt.title(f\"Meter readings for building #{bldg_id} \")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"meter_reading\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Delete eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "del eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ahKJL26YDPkA",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "heLVtwRqq4m2",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_df = reduce_mem_usage(pd.read_pickle('pkl/data_c32.pkl'))\n",
    "holidays_df = reduce_mem_usage(pd.read_pickle('pkl/holidays_df.pkl'))\n",
    "weather_df = reduce_mem_usage(pd.read_pickle('pkl/weather_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# adding a data column for merging\n",
    "data_df['date'] = data_df['timestamp'].dt.date.astype(np.datetime64)\n",
    "\n",
    "data_mer = data_df.set_index('date').join(holidays_df.set_index('date'), on='date', how='left').reset_index()\n",
    "\n",
    "data_mer.drop(labels=['name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_merge = data_mer.join(weather_df.set_index('date'), on='date', how='left').reset_index()\n",
    "data_merge.drop(labels=['date', 'index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fill the holiday column na with zeros\n",
    "data_merge['bank_holiday'].fillna(np.int64(0), inplace=True)\n",
    "data_merge['bank_holiday'] = data_merge['bank_holiday'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Spliting and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3378,
     "status": "ok",
     "timestamp": 1575376485975,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "Nvywn29vm-OJ",
    "outputId": "2039100d-70a5-4e2e-e947-edcc585cff61"
   },
   "outputs": [],
   "source": [
    "# change the site_id to numbers first\n",
    "#data_merge['site_id'] = data_merge.site_id.replace(['A01', 'A02', 'A03', 'A18', 'B02', 'B08', 'C02', 'C14', 'C19',\n",
    "#       'C24', 'C32', 'D08', 'D15', 'D20', 'D25', 'F27'], [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "data_merge['site_id'] = data_merge.site_id.replace(['a01'], [1])\n",
    "    \n",
    "X = data_merge.values\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "traindf, testdf = X[0:train_size], X[train_size:len(X)]\n",
    "print('Number of Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(traindf)))\n",
    "print('Testing Observations: %d' % (len(testdf)))\n",
    "\n",
    "# for aggregated data\n",
    "#train_df = pd.DataFrame(traindf, columns=['meter_reading', 'time', 'site_id', 'month', 'week_of_year', 'day_of_year', 'hour_of_day', 'day_of_week', 'day_of_month', 'week_of_month', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])\n",
    "#test_df = pd.DataFrame(testdf, columns=['meter_reading', 'time', 'site_id', 'month', 'week_of_year', 'day_of_year', 'hour_of_day', 'day_of_week', 'day_of_month', 'week_of_month', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])\n",
    "#test_val = pd.DataFrame(testdf, columns=['meter_reading', 'time', 'site_id', 'month', 'week_of_year', 'day_of_year', 'hour_of_day', 'day_of_week', 'day_of_month', 'week_of_month', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])\n",
    "\n",
    "\n",
    "# for one house\n",
    "train_df = pd.DataFrame(traindf, columns=['meter_reading', 'time', 'site_id', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])\n",
    "test_df = pd.DataFrame(testdf, columns=['meter_reading', 'time', 'site_id', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])\n",
    "test_val = pd.DataFrame(testdf, columns=['meter_reading', 'time', 'site_id', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1575376488065,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "9YWpylIi5xUh",
    "outputId": "a0b073f5-7b3e-45b2-9385-59e3d30a4290"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns='site_id', axis = 1)\n",
    "test_df = test_df.drop(columns='site_id', axis = 1)\n",
    "\n",
    "# changing the data types\n",
    "convert_dict = {#'site_id' : int,\n",
    "                #'month': int,\n",
    "                #'week_of_year': int,\n",
    "                #'day_of_year': int,\n",
    "                #'hour_of_day': int,\n",
    "                #'day_of_week': int,\n",
    "                #'day_of_month': int,\n",
    "                #'week_of_month': int,\n",
    "                'meter_reading': float,\n",
    "                'bank_holiday': int,\n",
    "                'max_air_temp': float,\n",
    "                'min_air_temp': float,\n",
    "                'mean_air_temp': float\n",
    "               }\n",
    "\n",
    "\n",
    "train_df = train_df.astype(convert_dict)\n",
    "test_df = test_df.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "1w8VEGM1hBxR"
   },
   "outputs": [],
   "source": [
    "# encoding the categorical columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "#traindf['site_id'] = le.fit_transform(traindf['site_id'])\n",
    "#testdf['site_id'] = le.fit_transform(testdf['site_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "categorical_feat = ['site_id']\n",
    "\n",
    "drop_cols = ['time', 'month', 'week_of_year', 'day_of_month', 'week_of_month', 'bank_holiday']\n",
    "\n",
    "numericals = ['day_of_year', 'hour_of_day',\t'day_of_week', 'max_air_temp', 'min_air_temp', 'mean_air_temp']\n",
    "\n",
    "feat_cols = categorical_feat + numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1395,
     "status": "ok",
     "timestamp": 1575376494373,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "bjllv-GcXSly",
    "outputId": "8b11c204-90e0-4c1d-e72b-4a78be9ccd07"
   },
   "outputs": [],
   "source": [
    "# target and features columns\n",
    "target = np.log1p(train_df['meter_reading'])\n",
    "\n",
    "del train_df[\"meter_reading\"] \n",
    "\n",
    "train_df = train_df.drop(drop_cols, axis = 1)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "QhhsBM6GnQNf"
   },
   "outputs": [],
   "source": [
    "# filling the na in meter_reading with zero\n",
    "target.fillna(0.09, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Simple Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Loading the data and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# only a01\n",
    "ts_df = data_df.drop(labels=['site_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df = ts_df.rename(columns = {'timestamp': 'ds', 'meter_reading': 'ts'})\n",
    "\n",
    "# Group data by number of listings per date\n",
    "df_example = df.groupby(by = 'ds').agg({'ts': 'sum'})\n",
    "\n",
    "# Change index to datetime\n",
    "df_example.index = pd.to_datetime(df_example.index)\n",
    "\n",
    "# Set frequency of time series\n",
    "df_example = df_example.asfreq(freq='1H')\n",
    "\n",
    "# Sort the values\n",
    "df_example = df_example.sort_index(ascending = True)\n",
    "\n",
    "# Fill values with 0\n",
    "df_example = df_example.fillna(value = 0)\n",
    "\n",
    "# Show the end of the data\n",
    "display(df_example.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Plot time series data\n",
    "f, ax = plt.subplots(1,1)\n",
    "ax.plot(df_example['ts'])\n",
    "\n",
    "# Add title\n",
    "ax.set_title('Time-series graph for a01 site (agg-1H)')\n",
    "\n",
    "# Rotate x-labels\n",
    "ax.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "# Show graph\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Testing for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def test_stationarity(df, ts, win = 12):\n",
    "    \"\"\"\n",
    "    Test stationarity using moving average statistics and Dickey-Fuller test\n",
    "    df is the dataframe\n",
    "    ts is the timestamp column\n",
    "    win is the window number for calculating the rolling mean ans std\n",
    "    \"\"\"\n",
    "    # Determing rolling statistics\n",
    "    rolmean = df[ts].rolling(window = win, center = False).mean()\n",
    "    rolstd = df[ts].rolling(window = win, center = False).std()\n",
    "    \n",
    "    # Plot rolling statistics:\n",
    "    orig = plt.plot(df[ts], color = 'blue', label = 'Original')\n",
    "    mean = plt.plot(rolmean, color = 'red', label = 'Rolling Mean')\n",
    "    std = plt.plot(rolstd, color = 'black', label = 'Rolling Std')\n",
    "    \n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show(block = False)\n",
    "    plt.close()\n",
    "    \n",
    "    # Perform Dickey-Fuller test:\n",
    "    # Null Hypothesis (H_0): time series is not stationary\n",
    "    # Alternate Hypothesis (H_1): time series is stationary\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(df[ts], autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistic','p-value','# Lags Used','Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_stationarity(df = df_example, ts = 'ts', win = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Moving average and other transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_transformed_data(df, ts, ts_transform):\n",
    "  \"\"\"\n",
    "  Plot transformed and original time series data\n",
    "  df is the dataframe\n",
    "  ts is the timestamp column\n",
    "  ts_transform is the name of transformed target column\n",
    "  \"\"\"\n",
    "  # Plot time series data\n",
    "  f, ax = plt.subplots(1,1)\n",
    "  ax.plot(df[ts])\n",
    "  ax.plot(df[ts_transform], color = 'red')\n",
    "\n",
    "  # Add title\n",
    "  ax.set_title('%s and %s time-series graph' %(ts, ts_transform))\n",
    "\n",
    "  # Rotate x-labels\n",
    "  ax.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "  # Add legend\n",
    "  ax.legend([ts, ts_transform])\n",
    "  \n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Transformation - log ts\n",
    "df_example['ts_log'] = df_example['ts'].apply(lambda x: np.log(x))\n",
    "\n",
    "# Transformation - 7-day moving averages of log ts\n",
    "df_example['ts_log_moving_avg'] = df_example['ts_log'].rolling(window = 7, center = False).mean()\n",
    "\n",
    "# Transformation - 7-day moving average ts\n",
    "df_example['ts_moving_avg'] = df_example['ts'].rolling(window = 7, center = False).mean()\n",
    "\n",
    "# Transformation - Difference between logged ts and first-order difference logged ts\n",
    "df_example['ts_log_diff'] = df_example['ts_log'] - df_example['ts_log'].shift()\n",
    "df_example['ts_log_diff'] = df_example['ts_log'].diff()\n",
    "\n",
    "# Transformation - Difference between ts and moving average ts\n",
    "df_example['ts_moving_avg_diff'] = df_example['ts'] - df_example['ts_moving_avg']\n",
    "\n",
    "# Transformation - Difference between logged ts and logged moving average ts\n",
    "df_example['ts_log_moving_avg_diff'] = df_example['ts_log'] - df_example['ts_log_moving_avg']\n",
    "\n",
    "# Transformation - Difference between logged ts and logged moving average ts\n",
    "df_example_transform = df_example.dropna()\n",
    "\n",
    "# Transformation - Logged exponentially weighted moving averages (EWMA) ts\n",
    "df_example_transform['ts_log_ewma'] = df_example_transform['ts_log'].ewm(halflife = 7, ignore_na = False, min_periods = 0, adjust = True).mean()\n",
    "\n",
    "# Transformation - Difference between logged ts and logged EWMA ts\n",
    "df_example_transform['ts_log_ewma_diff'] = df_example_transform['ts_log'] - df_example_transform['ts_log_ewma']\n",
    "\n",
    "# Display data\n",
    "display(df_example.head())\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = df_example_transform, ts = 'ts', ts_transform = 'ts_log')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts_log', ts_transform = 'ts_log_moving_avg')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts', ts_transform = 'ts_moving_avg')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts_log', ts_transform = 'ts_log_diff')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts', ts_transform = 'ts_moving_avg_diff')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts_log', ts_transform = 'ts_log_moving_avg_diff')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts_log', ts_transform = 'ts_log_ewma')\n",
    "#plot_transformed_data(df = df_example_transform, ts = 'ts_log', ts_transform = 'ts_log_ewma_diff')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = df_example_transform, ts = 'ts_log')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_moving_avg')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_log_moving_avg')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_log_diff')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_moving_avg_diff')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_log_moving_avg_diff')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_log_ewma')\n",
    "#test_stationarity(df = df_example_transform, ts = 'ts_log_ewma_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Decomposition: trend, seasonality, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_decomposition(df, ts, trend, seasonal, residual):\n",
    "  \"\"\"\n",
    "  Plot time series data\n",
    "  \"\"\"\n",
    "\n",
    "  f, ((ax1, ax2, ax3, ax4)) = plt.subplots(4,1, figsize = (8, 10), sharex = True)\n",
    "\n",
    "  ax1.plot(df[ts], label = 'Original')\n",
    "  ax1.legend(loc = 'upper right')\n",
    "  ax1.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "  ax2.plot(df[trend], label = 'Trend')\n",
    "  ax2.legend(loc = 'upper right')\n",
    "  ax2.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "  ax3.plot(df[seasonal],label = 'Seasonality')\n",
    "  ax3.legend(loc = 'upper right')\n",
    "  ax3.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "  ax4.plot(df[residual], label = 'Residuals')\n",
    "  ax4.legend(loc = 'upper right')\n",
    "  ax4.tick_params(axis = 'x', rotation = 45)\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Show graph\n",
    "  plt.suptitle('Trend, Seasonal, and Residual Decomposition of %s' %(ts), x = 0.5, y = 1.05, fontsize = 13)\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(df_example_transform['ts_log'], freq = 365)\n",
    "\n",
    "df_example_transform.loc[:,'trend'] = decomposition.trend\n",
    "df_example_transform.loc[:,'seasonal'] = decomposition.seasonal\n",
    "df_example_transform.loc[:,'residual'] = decomposition.resid\n",
    "\n",
    "plot_decomposition(df = df_example_transform, ts = 'ts_log', trend = 'trend', seasonal = 'seasonal', residual = 'residual')\n",
    "\n",
    "test_stationarity(df = df_example_transform.dropna(), ts = 'residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ACF and PACF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_acf_pacf(df, ts):\n",
    "  \"\"\"\n",
    "  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n",
    "  \"\"\"\n",
    "  f, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5)) \n",
    "\n",
    "  #Plot ACF: \n",
    "\n",
    "  ax1.plot(lag_acf)\n",
    "  ax1.axhline(y=0,linestyle='--',color='gray')\n",
    "  ax1.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
    "  ax1.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
    "  ax1.set_title('Autocorrelation Function for %s' %(ts))\n",
    "\n",
    "  #Plot PACF:\n",
    "  ax2.plot(lag_pacf)\n",
    "  ax2.axhline(y=0,linestyle='--',color='gray')\n",
    "  ax2.axhline(y=-1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
    "  ax2.axhline(y=1.96/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n",
    "  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# determine ACF and PACF\n",
    "lag_acf = acf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\n",
    "lag_pacf = pacf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\n",
    "\n",
    "# plot ACF and PACF\n",
    "plot_acf_pacf(df = df_example_transform, ts = 'ts_log_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def run_arima_model(df, ts, p, d, q):\n",
    "  \"\"\"\n",
    "  Run ARIMA model\n",
    "  p is the number of lags\n",
    "  \"\"\"\n",
    "\n",
    "  # fit ARIMA model on time series\n",
    "  model = ARIMA(df[ts], order=(p, d, q))  \n",
    "  results_ = model.fit(disp=-1)  \n",
    "  \n",
    "  # get lengths correct to calculate RSS\n",
    "  len_results = len(results_.fittedvalues)\n",
    "  ts_modified = df[ts][-len_results:]\n",
    "  \n",
    "  # calculate root mean square error (RMSE) and residual sum of squares (RSS)\n",
    "  rss = sum((results_.fittedvalues - ts_modified)**2)\n",
    "  rmse = np.sqrt(rss / len(df[ts]))\n",
    "  \n",
    "  # plot fit\n",
    "  plt.plot(df[ts])\n",
    "  plt.plot(results_.fittedvalues, color = 'red')\n",
    "  plt.title('For ARIMA model (%i, %i, %i) for ts %s, RSS: %.4f, RMSE: %.4f' %(p, d, q, ts, rss, rmse))\n",
    "  \n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Note: I do the differencing in the transformation of the data 'ts_log_diff'\n",
    "# AR model with 1st order differencing - ARIMA (1,0,0)\n",
    "#model_AR = run_arima_model(df = df_example_transform, ts = 'ts_log_diff', p = 1, d = 0, q = 0)\n",
    "\n",
    "# MA model with 1st order differencing - ARIMA (0,0,1)\n",
    "#model_MA = run_arima_model(df = df_example_transform, ts = 'ts_log_diff', p = 0, d = 0, q = 1)\n",
    "\n",
    "# ARMA model with 1st order differencing - ARIMA (1,0,1)\n",
    "model_ARMA = run_arima_model(df = df_example_transform, ts = 'ts_log', p = 5, d = 0, q = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Taking it back to original scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(model_ARMA.fittedvalues, copy=True)\n",
    "#predictions_ARIMA_diff.head()\n",
    "\n",
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "#predictions_ARIMA_diff_cumsum.head()\n",
    "\n",
    "predictions_ARIMA_log = pd.Series(df_example_transform['ts_log'], index=df_example_transform.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()\n",
    "\n",
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "#plt.plot(df_example)\n",
    "plt.plot(predictions_ARIMA)\n",
    "#plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-df_example_transform['ts'])**2)/len(ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### FBProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def days_between(d1, d2):\n",
    "    \"\"\"Calculate the number of days between two dates.  D1 is start date (inclusive) and d2 is end date (inclusive)\"\"\"\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return abs((d2 - d1).days + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Inputs for query\n",
    "\n",
    "date_column = 'dt'\n",
    "metric_column = 'ts'\n",
    "table = df_example\n",
    "start_training_date = '2010-07-03'\n",
    "end_training_date = '2018-09-08'\n",
    "start_forecasting_date = '2018-09-09'\n",
    "end_forecasting_date = '2018-12-31'\n",
    "year_to_estimate = '2018'\n",
    "\n",
    "# Inputs for forecasting\n",
    "\n",
    "# future_num_points\n",
    "# If doing different time intervals, change future_num_points\n",
    "future_num_points = days_between(start_forecasting_date, end_forecasting_date)\n",
    "\n",
    "cap = None # 2e6\n",
    "\n",
    "# growth: default = 'linear'\n",
    "# Can also choose 'logistic'\n",
    "growth = 'linear'\n",
    "\n",
    "# n_changepoints: default = 25, uniformly placed in first 80% of time series\n",
    "n_changepoints = 25 \n",
    "\n",
    "# changepoint_prior_scale: default = 0.05\n",
    "# Increasing it will make the trend more flexible\n",
    "changepoint_prior_scale = 0.05 \n",
    "\n",
    "# changpoints: example = ['2016-01-01']\n",
    "changepoints = None \n",
    "\n",
    "# holidays_prior_scale: default = 10\n",
    "# If you find that the holidays are overfitting, you can adjust their prior scale to smooth them\n",
    "holidays_prior_scale = 10 \n",
    "\n",
    "# interval_width: default = 0.8\n",
    "interval_width = 0.8 \n",
    "\n",
    "# mcmc_samples: default = 0\n",
    "# By default Prophet will only return uncertainty in the trend and observation noise.\n",
    "# To get uncertainty in seasonality, you must do full Bayesian sampling. \n",
    "# Replaces typical MAP estimation with MCMC sampling, and takes MUCH LONGER - e.g., 10 minutes instead of 10 seconds.\n",
    "# If you do full sampling, then you will see the uncertainty in seasonal components when you plot:\n",
    "mcmc_samples = 0\n",
    "\n",
    "# holiday: default = None\n",
    "# thanksgiving = pd.DataFrame({\n",
    "#   'holiday': 'thanksgiving',\n",
    "#   'ds': pd.to_datetime(['2014-11-27', '2015-11-26',\n",
    "#                         '2016-11-24', '2017-11-23']),\n",
    "#   'lower_window': 0,\n",
    "#   'upper_window': 4,\n",
    "# })\n",
    "# christmas = pd.DataFrame({\n",
    "#   'holiday': 'christmas',\n",
    "#   'ds': pd.to_datetime(['2014-12-25', '2015-12-25', \n",
    "#                         '2016-12-25','2017-12-25']),\n",
    "#   'lower_window': -1,\n",
    "#   'upper_window': 0,\n",
    "# })\n",
    "# holidays = pd.concat((thanksgiving,christmas))\n",
    "holidays = None\n",
    "\n",
    "daily_seasonality = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get relevant data - note: could also try this with ts_log_diff\n",
    "df_prophet = df_example_transform[['ts']] # can try with ts_log_diff\n",
    "\n",
    "# reset index\n",
    "df_prophet = df_prophet.reset_index()\n",
    "\n",
    "# rename columns\n",
    "df_prophet = df_prophet.rename(columns = {'ds': 'ds', 'ts': 'y'}) # can try with ts_log_diff\n",
    "\n",
    "# Change 'ds' type from datetime to date (necessary for FB Prophet)\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "\n",
    "# Change 'y' type to numeric (necessary for FB Prophet)\n",
    "df_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n",
    "\n",
    "# Remove any outliers\n",
    "# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_daily_forecast(df,\n",
    "#                           cap,\n",
    "                          holidays,\n",
    "                          growth,\n",
    "                          n_changepoints = 25,\n",
    "                          changepoint_prior_scale = 0.05,\n",
    "                          changepoints = None,\n",
    "                          holidays_prior_scale = 10,\n",
    "                          interval_width = 0.8,\n",
    "                          mcmc_samples = 1,\n",
    "                          future_num_points = 10, \n",
    "                          daily_seasonality = True):\n",
    "  \"\"\"\n",
    "  Create forecast\n",
    "  \"\"\"\n",
    "  \n",
    "  # Create copy of dataframe\n",
    "  df_ = df.copy()\n",
    "\n",
    "  # Add in growth parameter, which can change over time\n",
    "  #     df_['cap'] = max(df_['y']) if cap is None else cap\n",
    "\n",
    "  # Create model object and fit to dataframe\n",
    "  m = Prophet(growth = growth,\n",
    "              n_changepoints = n_changepoints,\n",
    "              changepoint_prior_scale = changepoint_prior_scale,\n",
    "              changepoints = changepoints,\n",
    "              holidays = holidays,\n",
    "              holidays_prior_scale = holidays_prior_scale,\n",
    "              interval_width = interval_width,\n",
    "              mcmc_samples = mcmc_samples, \n",
    "              daily_seasonality = daily_seasonality)\n",
    "\n",
    "  # Fit model with dataframe\n",
    "  m.fit(df_)\n",
    "\n",
    "  # Create dataframe for predictions\n",
    "  future = m.make_future_dataframe(periods = future_num_points)\n",
    "  #     future['cap'] = max(df_['y']) if cap is None else cap\n",
    "\n",
    "  # Create predictions\n",
    "  fcst = m.predict(future)\n",
    "\n",
    "  # Plot\n",
    "  m.plot(fcst);\n",
    "  m.plot_components(fcst)\n",
    "\n",
    "  return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fcst = create_daily_forecast(df_prophet,\n",
    "#                              cap,\n",
    "                             holidays,\n",
    "                             growth,\n",
    "                             n_changepoints,\n",
    "                             changepoint_prior_scale,\n",
    "                             changepoints, \n",
    "                             holidays_prior_scale,\n",
    "                             interval_width,\n",
    "                             mcmc_samples,\n",
    "                             future_num_points, \n",
    "                             daily_seasonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7fqZsgH0jXC2"
   },
   "source": [
    "## LGBM (Gradient Boosting Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6377,
     "status": "ok",
     "timestamp": 1575376520053,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "AICyibD9YYCL",
    "outputId": "d822bd8c-f0c9-475c-811d-5b966371792d"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.3,\n",
    "            'bagging_freq': 5,\n",
    "            'num_leaves': 330,\n",
    "            'feature_fraction': 0.9,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }\n",
    "\n",
    "folds = 10\n",
    "seed = 666\n",
    "shuffle = False\n",
    "kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n",
    "\n",
    "models = []\n",
    "\n",
    "for train_index,test_index in kf.split(train_df[feat_cols]):\n",
    "    train_features = train_df[feat_cols].loc[train_index]\n",
    "    train_target = target.loc[train_index]\n",
    "    \n",
    "    test_features = train_df[feat_cols].loc[test_index]\n",
    "    test_target = target.loc[test_index]\n",
    "    \n",
    "    d_training = lgb.Dataset(train_features, label=train_target, categorical_feature=categorical_feat, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(test_features, label=test_target, categorical_feature=categorical_feat, free_raw_data=False)\n",
    "    \n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_training,\n",
    "                      num_boost_round=1000,\n",
    "                      valid_sets=[d_training,d_test],\n",
    "                      verbose_eval=25,\n",
    "                      early_stopping_rounds=50)\n",
    "    \n",
    "    models.append(model)\n",
    "    \n",
    "    del train_features, train_target, test_features, test_target, d_training, d_test\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2823,
     "status": "ok",
     "timestamp": 1575376526833,
     "user": {
      "displayName": "Ashkan Lotfipoor",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAEm2E9r6DKqrpIOsevMp1G26uppYAOpx5wj_c3Qg=s64",
      "userId": "14870018713798350617"
     },
     "user_tz": 0
    },
    "id": "3CW9AWl4phUc",
    "outputId": "a4c528af-4ba6-4f38-aad3-5f57dc7cb5eb"
   },
   "outputs": [],
   "source": [
    "# feature Importance# \n",
    "for model in models:\n",
    "    lgb.plot_importance(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_df = test_df.drop(drop_cols, axis = 1)\n",
    "testtarget = test_df['meter_reading']\n",
    "test_df = test_df.drop('meter_reading', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "RiJgRmeKtBUJ"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "results = []\n",
    "for model in models:\n",
    "    if  results == []:\n",
    "        results = np.expm1(model.predict(test_df, num_iteration=model.best_iteration)) / len(models)\n",
    "    else:\n",
    "        results += np.expm1(model.predict(test_df, num_iteration=model.best_iteration)) / len(models)\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "compare = pd.DataFrame(results, columns=['lgbm_pred'])\n",
    "compare = compare.join(test_val, how='left')\n",
    "compare = compare.drop(['month', 'week_of_year', 'day_of_year', 'hour_of_day', 'day_of_week', 'day_of_month', 'week_of_month', 'bank_holiday', 'max_air_temp', 'min_air_temp', 'mean_air_temp'] , axis = 1)\n",
    "compare['meter_loglp'] = np.log1p(testtarget)\n",
    "compare['dif_lgbm'] = compare['meter_loglp'] - compare['lgbm_pred']\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Deep Neural Netwoork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# use for imputining data points with mean.\n",
    "\n",
    "def average_imputation(df, column_name):\n",
    "    imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "    del imputation\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.001):\n",
    "\n",
    "    #Inputs\n",
    "    site_id = Input(shape=[1], name=\"site_id\")\n",
    "    day_of_year = Input(shape=[1], name=\"day_of_year\")\n",
    "    hour_of_day = Input(shape=[1], name=\"hour_of_day\")\n",
    "    day_of_week = Input(shape=[1], name=\"day_of_week\")\n",
    "    max_air_temp = Input(shape=[1], name=\"max_air_temp\")\n",
    "    min_air_temp = Input(shape=[1], name=\"min_air_temp\")\n",
    "    mean_air_temp = Input(shape=[1], name=\"mean_air_temp\")\n",
    "   \n",
    "    #Embeddings layers\n",
    "    emb_site_id = Embedding(16, 2)(site_id)\n",
    "    emb_day_of_year = Embedding(365, 2)(day_of_year)\n",
    "    emb_hour_of_day = Embedding(24, 3)(hour_of_day)\n",
    "    emb_day_of_week = Embedding(7, 2)(day_of_week)\n",
    "\n",
    "    concat_emb = concatenate([Flatten() (emb_site_id), Flatten() (emb_day_of_year), Flatten() (emb_hour_of_day), Flatten() (emb_day_of_week)])\n",
    "    \n",
    "    categ = Dropout(dropout1)(Dense(dense_dim_1,activation='relu') (concat_emb))\n",
    "    categ = BatchNormalization()(categ)\n",
    "    categ = Dropout(dropout2)(Dense(dense_dim_2,activation='relu') (categ))\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([categ, max_air_temp, min_air_temp, mean_air_temp])\n",
    "    main_l = Dropout(dropout3)(Dense(dense_dim_3,activation='relu') (main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    main_l = Dropout(dropout4)(Dense(dense_dim_4,activation='relu') (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1) (main_l)\n",
    "    \n",
    "    model = Model([site_id, day_of_year, hour_of_day, day_of_week, max_air_temp, min_air_temp, mean_air_temp], output)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=lr), loss= mse_loss, metrics=[root_mean_squared_error])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    cols = num_cols + cat_cols\n",
    "    X = {col: np.array(df[col]) for col in cols}\n",
    "    return X\n",
    "\n",
    "def train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold, patience=3):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint(\"model_\" + str(fold) + \".hdf5\",\n",
    "                                       save_best_only=True, verbose=1, monitor='val_root_mean_squared_error', mode='min')\n",
    "\n",
    "    hist = keras_model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=(X_v, y_valid), verbose=1,\n",
    "                            callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    keras_model = load_model(\"model_\" + str(fold) + \".hdf5\", custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof = np.zeros(len(train_df))\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "models = []\n",
    "\n",
    "folds = 5\n",
    "seed = 666\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(kf.split(train_df, train_df['site_id'])):\n",
    "    print('Fold:', fold_n)\n",
    "    X_train, X_valid = train_df.iloc[train_index], train_df.iloc[valid_index]\n",
    "    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "    X_t = get_keras_data(X_train, numericals, categorical_feat)\n",
    "    X_v = get_keras_data(X_valid, numericals, categorical_feat)\n",
    "    \n",
    "    keras_model = model(dense_dim_1=64, dense_dim_2=32, dense_dim_3=32, dense_dim_4=16, \n",
    "                        dropout1=0.2, dropout2=0.1, dropout3=0.1, dropout4=0.1, lr=0.01)\n",
    "    mod = train_model(keras_model, X_t, y_train, batch_size, epochs, X_v, y_valid, fold_n, patience=3)\n",
    "    models.append(mod)\n",
    "    print('*'* 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res = np.zeros((test_df.shape[0]),dtype=np.float32)\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test_df.shape[0]/step_size)))):\n",
    "    for_prediction = get_keras_data(test_df.iloc[i:i+step_size], numericals, categorical_feat)\n",
    "    res[i:min(i+step_size,test_df.shape[0])] = \\\n",
    "       np.expm1(sum([model.predict(for_prediction, batch_size=1024)[:,0] for model in models])/folds)\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "compare['NN_pred'] = res\n",
    "compare.loc[compare['meter_reading']<0, 'meter_reading'] = 0\n",
    "compare['dif_NN'] = compare['meter_reading'] - compare['NN_pred']\n",
    "compare.to_csv('csv/compre_result.csv', index=False)\n",
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(compare['time'], compare['meter_reading'])\n",
    "plt.plot(compare['time'], compare['lgbm_pred'], color='r')\n",
    "plt.plot(compare['time'], compare['NN_pred'], color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#lstm_df = df_example.drop(labels=['ts_log', 'ts_log_moving_avg', 'ts_moving_avg', 'ts_log_diff', 'ts_moving_avg_diff', 'ts_log_moving_avg_diff'], axis=1)\n",
    "lstm_df = df_example.copy()\n",
    "lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def do_lstm_model(df, ts, look_back, epochs, type_ = None, train_fraction = 0.80):\n",
    "  \"\"\"\n",
    "   Create LSTM model\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert an array of values into a dataset matrix\n",
    "  def create_dataset(dataset, look_back=1):\n",
    "    \"\"\"\n",
    "    Create the dataset\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "      a = dataset[i:(i+look_back), 0]\n",
    "      dataX.append(a)\n",
    "      dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "  # Get dataset\n",
    "  dataset = df[ts].values\n",
    "  dataset = dataset.astype('float32')\n",
    "\n",
    "  # Normalize the dataset\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "  \n",
    "  # Split into train and test sets\n",
    "  train_size = int(len(dataset) * train_fraction)\n",
    "  test_size = len(dataset) - train_size\n",
    "  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "  \n",
    "  # Reshape into X=t and Y=t+1\n",
    "  look_back = look_back\n",
    "  trainX, trainY = create_dataset(train, look_back)\n",
    "  testX, testY = create_dataset(test, look_back)\n",
    "  \n",
    "  # Reshape input to be [samples, time steps, features]\n",
    "  if type_ == 'regression with time steps':\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "  elif type_ == 'stacked with memory between batches':\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "  else:\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "  \n",
    "  # Create and fit the LSTM network\n",
    "  batch_size = 1\n",
    "  model = Sequential()\n",
    "  \n",
    "  if type_ == 'regression with time steps':\n",
    "    model.add(LSTM(6, input_shape=(look_back, 1)))\n",
    "  elif type_ == 'memory between batches':\n",
    "    model.add(LSTM(6, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "  elif type_ == 'stacked with memory between batches':\n",
    "    model.add(LSTM(6, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "    model.add(LSTM(8, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "  else:\n",
    "    model.add(LSTM(6, input_shape=(1, look_back)))\n",
    "  \n",
    "  model.add(Dense(1, activation='relu'))\n",
    "  model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
    "    for i in range(100):\n",
    "      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "      model.reset_states()\n",
    "  else:\n",
    "    model.fit(trainX, \n",
    "              trainY, \n",
    "              epochs = epochs, \n",
    "              batch_size = 1, \n",
    "              verbose = 2)\n",
    "  \n",
    "  # Make predictions\n",
    "  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n",
    "    trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "    testPredict = model.predict(testX, batch_size=batch_size)\n",
    "  else:\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "  \n",
    "  # Invert predictions\n",
    "  trainPredict = scaler.inverse_transform(trainPredict)\n",
    "  trainY = scaler.inverse_transform([trainY])\n",
    "  testPredict = scaler.inverse_transform(testPredict)\n",
    "  testY = scaler.inverse_transform([testY])\n",
    "  \n",
    "  # Calculate root mean squared error\n",
    "  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "  print('Train Score: %.2f RMSE' % (trainScore))\n",
    "  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "  print('Test Score: %.2f RMSE' % (testScore))\n",
    "  \n",
    "  # Shift train predictions for plotting\n",
    "  trainPredictPlot = np.empty_like(dataset)\n",
    "  trainPredictPlot[:, :] = np.nan\n",
    "  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "  \n",
    "  # Shift test predictions for plotting\n",
    "  testPredictPlot = np.empty_like(dataset)\n",
    "  testPredictPlot[:, :] = np.nan\n",
    "  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "  \n",
    "  model.summary()\n",
    "    \n",
    "  # Plot baseline and predictions\n",
    "  plt.plot(scaler.inverse_transform(dataset))\n",
    "  plt.plot(trainPredictPlot)\n",
    "  plt.plot(testPredictPlot)\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.clock()\n",
    "\n",
    "# LSTM Network for Regression\n",
    "trainX, trainY, testX, testY = do_lstm_model(df = df_example, ts = 'ts', look_back = 3, epochs = 5)\n",
    "\n",
    "# LSTM for Regression Using the Window Method\n",
    "#do_lstm_model(df = df_example, ts = 'ts', look_back = 10, epochs = 10)\n",
    "\n",
    "# LSTM for Regression with Time Steps\n",
    "#do_lstm_model(df = df_example, ts = 'ts', look_back = 3, epochs = 5, type_ = 'regression with time steps')\n",
    "\n",
    "# LSTM with Memory Between Batches\n",
    "#do_lstm_model(df = df_example, ts = 'ts', look_back = 3, epochs = 5, type_ = 'memory between batches')\n",
    "\n",
    "# Stacked LSTMs with Memory Between Batches\n",
    "#do_lstm_model(df = df_example, ts = 'ts', look_back = 30, epochs = 10, type_ = 'stacked with memory between batches')\n",
    "\n",
    "toc = time.clock()\n",
    "timeit = (toc - tic)/60\n",
    "print('It takes ',str(round(timeit, 2)),' Min to train and test the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_X = trainX.reshape(trainX.shape[0], trainX.shape[2])\n",
    "train_Y = trainY.reshape(trainY.shape[1])\n",
    "test_X = testX.reshape(testX.shape[0], testX.shape[2])\n",
    "test_Y = testY.reshape(testY.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Perform Grid-Search\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid={\n",
    "        'max_depth': range(3,7),\n",
    "        'n_estimators': (10, 50, 100, 1000),\n",
    "    },\n",
    "    cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "grid_result = gsc.fit(train_X, train_Y)\n",
    "best_params = grid_result.best_params_\n",
    "    \n",
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"], random_state=False, verbose=False)\n",
    "\n",
    "scores = cross_val_score(rfr, train_X, train_Y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ekg3u_1eDPiQ",
    "FoUxQN1qbKTZ"
   ],
   "name": "EDA.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/alotfipoor/phd/blob/master/EDA.ipynb",
     "timestamp": 1575287540477
    },
    {
     "file_id": "1ZOkJ0lB222RfEz3QPESR4oHhT-GHwne-",
     "timestamp": 1574686816955
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "deep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
